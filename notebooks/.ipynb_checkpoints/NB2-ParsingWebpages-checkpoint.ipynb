{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\" <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@600&family=Noto+Sans+JP&display=swap\" rel=\"stylesheet\"> \n",
    "<style>\n",
    "    div.text_cell_render h1 {\n",
    "        font-family: 'Inter';\n",
    "        font-size: 1.7em;\n",
    "        line-height:1.4em;\n",
    "        text-align:center;\n",
    "        }\n",
    "\n",
    "    div.text_cell_render { \n",
    "        font-family: 'Noto Sans JP';\n",
    "        font-size:1.05em;\n",
    "        line-height:1.5em;\n",
    "        padding-left:3em;\n",
    "        padding-right:3em;\n",
    "        }\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Web Data\n",
    "\n",
    "In the previous notebook we identified webpages that host a specific image. The URLs that refer to these webpages are found in .json files. In this notebook, we use the URLs to download the actual pages, extract their textual content and prepare the textual data for analysis. Also, we extract general features of the webpages (the metadata) for analysis. To get an idea of the .json files, we start by opening them and inspecting the list of URLs. \n",
    "\n",
    "First, we define some basic variables. Then we gather the list of .json files and open the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "\n",
    "base_path = \"D:/test/\" \n",
    "photo = \"9\"\n",
    "photo_folder = os.path.join(base_path, photo)\n",
    "\n",
    "# Check how many iterations we have by using the os.listdir function. We don't want the \"source\" folder because it doesn't contain jsons\n",
    "num_iterations = len([fol for fol in os.listdir(photo_folder) if os.path.isdir(os.path.join(photo_folder,fol)) and \"source\" not in fol])\n",
    "start_iter = 1\n",
    "range_iter = [str(i) for i in list(range(1,num_iterations+1))]\n",
    "\n",
    "list_jsons = []\n",
    "\n",
    "# We now \"loop\" through the folders associated with the iterations and gather the .jsons in these folders\n",
    "for iteration in range_iter:\n",
    "    iteration_folder = os.path.join(photo_folder, photo + \"_\" + str(iteration))\n",
    "    list_json_in_iteration_folder = [os.path.join(iteration_folder,js) for js in os.listdir(iteration_folder) if \".json\" in js]\n",
    "    print(\"Found {} .json files in {}\".format(len(list_json_in_iteration_folder),iteration_folder))\n",
    "    list_jsons += list_json_in_iteration_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can open the .json files by loading them with the json module that comes automatically with your Python installation. You can inspect or \"walk\" the data by selecting keys with names (```json_data['responses']```) or elements in lists (```json_data['responses'][0:10]```). To find the URLs, navigate to the ```pagesWithMatchingImages``` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(list_jsons[0],'r') as fr:\n",
    "    json_data = json.load(fr)\n",
    "\n",
    "# Show the first elements in the list:\n",
    "json_data['responses'][0]['webDetection']['pagesWithMatchingImages'][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the json data consists of key:value pairs. In the 'pagesWithMatchingImages' list you can find:\n",
    "1. The title of the page where the image is found\n",
    "2. The link to the image file (www.example.com/media/examplephoto.png). This can be either a 'partialMatchingImage' or a 'fullMatchingImage'. The difference between the two is hard to explain, but in most cases a \"fullMatch\" concerns a copy of the input image, only different in scale or quality. A 'partialMatch' usually consists of for example images where the input image is only part of (for example an image of a t-shirt that includes a print of the input image.\n",
    "3. The link to the page itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Pipeline\n",
    "\n",
    "To work with the metadata and text data associated with the webpages we need to extract, clean and harmonize this data. We do so by:\n",
    "- downloading the webpages in .html format\n",
    "- extract the text from the .html pages\n",
    "- identify languages\n",
    "- identify dates\n",
    "- identify Named Entities\n",
    "\n",
    "Because these individual steps require a lot of code, we decided to import some classes and functions that do most of the work. If you want to see what happens when we parse the texts, languages, dates and entities, just open ```functions.py``` in the notebook folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "photo_folder = os.path.join(base_path, photo)\n",
    "num_iterations = len([fol for fol in os.listdir(photo_folder) if os.path.isdir(os.path.join(photo_folder,fol)) and \"source\" not in fol])\n",
    "start_iter = 1\n",
    "range_iter = [str(i) for i in list(range(1,num_iterations+1))]\n",
    "folder_base = os.path.join(photo_folder,photo)\n",
    "\n",
    "for iteration in range_iter:\n",
    "    \n",
    "    # Import page URLs from .json files (using Json.extract_pages())\n",
    "    iteration_path = folder_base + \"_\" + str(iteration)\n",
    "    jsFiles = [js for js in os.listdir(iteration_path) if \".json\" in js]\n",
    "\n",
    "    list_urls = []\n",
    "    for js in jsFiles:\n",
    "        json_file_path = os.path.join(folder_base + \"_\" + str(iteration), js)\n",
    "        list_urls += Json.extract_pages(json_file_path)\n",
    "            \n",
    "    # Import previously scraped URLs if iteration number is larger than 1:\n",
    "    scraped_urls = []\n",
    "    if int(iteration) > 1:\n",
    "        for i in range(1,int(iteration)):\n",
    "            try:\n",
    "                with open(os.path.join(folder_base + \"_\" + str(i), \"html\",\"results.txt\"), 'r', encoding='utf-8') as f:\n",
    "                    print(\"INFO: importing from {}\".format(os.path.join(folder_base + \"_\" + str(i), \"html\",\"results.txt\")))\n",
    "                    lu = f.readlines()\n",
    "                    lu = [l.split('|') for l in lu]\n",
    "                    lu = [l for l in lu if len(l) == 2]\n",
    "                    lu = [l[1].replace('\\n','') for l in lu]\n",
    "                    scraped_urls = scraped_urls + lu\n",
    "            except FileNotFoundError:\n",
    "                print(os.path.join(folder_base + \"_\" + str(i), \"html\",\"results.txt\"), \"not found\")\n",
    "\n",
    "    #Scrape All Page URLs to 'image[...]/html' folder\n",
    "    destination_path = os.path.join(folder_base + \"_\" + str(iteration), \"html\")\n",
    "    if not os.path.exists(destination_path):\n",
    "        os.makedirs(destination_path)\n",
    "    \n",
    "    list_urls = [u for u in list_urls if u not in scraped_urls]\n",
    "    print('INFO: Scraping {} URLs in iteration {}'.format(len(list_urls),iteration))\n",
    "\n",
    "    pagescraper.PoolScrape(list_urls, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now find a ```html``` folders inside the ```example_photo1_[iteration number]``` folders. The folder structure now looks like this:\n",
    "```\n",
    "+-- photo_folder\n",
    "    +-- example_photo_1_folder\n",
    "        +-- example_photo_1_1\n",
    "            +-- html (location of downloaded webpages)\n",
    "            +-- img (location of images used for gathering webpages)\n",
    "            +-- photo_name_identifier1.json (json files)\n",
    "            +-- photo_name_identifier2.json\n",
    "        +-- example_photo_1_2\n",
    "        +-- example_photo_1_3\n",
    "        +-- example_photo_1_folder_source\n",
    "```\n",
    "\n",
    "Next, we download the text to a ```txt``` folder that is going to be located at the same level as ```html``` and ```img```. In this folder, we gather all the webpage texts in one single .json file (for easy loading during the analysis). \n",
    "\n",
    "Extracting text from webpages is everything but a straightforward process, because it is not clear beforehand what is relevant. For example, text from ads or menubars are not relevant to the research. Luckily, there is software available that \"parses\" relevant texts. In this pipeline, we use a Python implementation of [boilerpipe](https://github.com/misja/python-boilerpipe) a piece of software that removes clutter and irrelevant bits from webpage texts. Boilerpipe offers various options. Inside the ```function.py``` you will find the setup of our use of the parser. To make things easier we handle the parsing in a separate function. Simply name the base path and the photo. Check the ```functions.py``` file to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Parse.Texts(base_path,photo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the URL text and the webpage text, we can now identifiy the language of the webpage. Here, we make use of ```langid``` a language identification library for Python. Below we identify the languages in a similar way as the we extracted the texts: by iterating over the folders. We write the languages to a .json file that is located in the main photo folder. The language identifier returns a probability score and the best guess language. First, a quick demonstration of the langid module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "print(identifier.classify(\"this is a test sentence that could be in english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape the languages, we need to open the .json file that contains the texts. Every iteration folder has such a .json. We thus need to iterate over the folders. Because we already know how to do this, we can use the predefined Parse.Languages() function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parse.Languages(base_path,photo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the languages written to a .json, we now repeat a similarly looking procedure for the dates. The ```htmldate``` module identifies the date of publication for a URL. Because the module depends on the avaiability of information embedded in the html, it does not cover all the URLs, but enough to get an idea of the temporal distribution of our data. We write the extracted dates to ```dates.json``` in the main photo folder. An example of the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import htmldate\n",
    "print(htmldate.find_date(\"http://www.eleggua.com/Objects/WTO/Death.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, running this function over all the URLs stored in different folders is handled in a predefined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parse.Dates(base_path, photo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we identify Named Entities in our text data. This method extracts entities (locations, persons, dates etc.) from the text, based on pretrained models. We use the popular Spacy models (available for english, dutch, italian, french, spanish, portugese) to do this. Named Entities can be used to study the context of certain keywords, and the phenomena associated with the photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "photo_folder = os.path.join(base_path, photo)\n",
    "num_iterations = len([fol for fol in os.listdir(photo_folder) if os.path.isdir(os.path.join(photo_folder,fol)) and \"source\" not in fol])\n",
    "start_iter = 1\n",
    "range_iter = [str(i) for i in list(range(1,num_iterations+1))]\n",
    "folder_base = os.path.join(base_path,photo,photo)\n",
    "#########################\n",
    "\n",
    "print(\"INFO: Named Entitiy Recognition using Spacy\")\n",
    "\n",
    "selected_languages = \"en de fr es it nl pt\".split(' ')\n",
    "selected_languages = {i:i+\"_core_news_sm\" for i in selected_languages}\n",
    "selected_languages.update({\"en\":\"en_core_web_sm\"})\n",
    "\n",
    "def PreProc(text):\n",
    "    text = text[1:-1].replace('\\xa0', ' ')\n",
    "    text = \" \".join(text.split('\\r\\n'))\n",
    "    return text\n",
    "\n",
    "d_ = {}\n",
    "for iteration in range_iter:\n",
    "    fn = os.path.join(folder_base + \"_\" +str(iteration),\"txt\", \"parsed_text.json\")\n",
    "    if os.path.isdir(os.path.join(folder_base + \"_\" +str(iteration),\"txt\")) == False:\n",
    "        print('INFO: no parsed text found in iteration {}'.format(iteration))\n",
    "        continue\n",
    "\n",
    "    with open(fn) as fp:\n",
    "        pages = json.load(fp)\n",
    "\n",
    "    df = []\n",
    "    for id_,sentences in pages.items():\n",
    "\n",
    "        sentences = [s.replace(\"\\n\",\"\").lower() for s in sentences]\n",
    "        sentences = [re.sub(' +', ' ', s) for s in sentences]\n",
    "\n",
    "        url = id_.split('html_')[-1]\n",
    "        id_ = id_.split('/html/')[1].split('.html_')[0]\n",
    "        language = identifier.classify(str(sentences))[0]\n",
    "        df.append([url,id_,language,str(sentences)])\n",
    "\n",
    "    df = pd.DataFrame(df,columns=['url','id','lang','text'])\n",
    "\n",
    "    # NER\n",
    "    for lang in [i for i in list(set(df['lang'])) if i in selected_languages.keys()]:\n",
    "        print('INFO: working on language: {}'.format(lang))\n",
    "        if lang not in d_.keys():\n",
    "            d_.update({lang:dict()})\n",
    "        nlp = spacy.load(selected_languages[lang])\n",
    "        tmp = df[df['lang'] == lang]\n",
    "\n",
    "        for count,text in enumerate(df['text']):\n",
    "            identif = str(df['id'][count])\n",
    "            d_[lang].update({identif:dict()})\n",
    "            d_[lang][identif].update({\"text\":text})\n",
    "            doc = nlp(text)\n",
    "            doc = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "            d_[lang][identif].update({\"entities\":doc})\n",
    "\n",
    "with open(os.path.join(base_path, photo,\"entities-{}.json\".format(photo)), 'w') as fp:\n",
    "    json.dump(d_, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
