{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@600&family=Noto+Sans+JP&display=swap\" rel=\"stylesheet\"> \n",
       "<style>\n",
       "    div.text_cell_render h1 {\n",
       "        font-family: 'Inter';\n",
       "        font-size: 1.7em;\n",
       "        line-height:1.4em;\n",
       "        text-align:center;\n",
       "        }\n",
       "\n",
       "    div.text_cell_render { \n",
       "        font-family: 'Noto Sans JP';\n",
       "        font-size:1.05em;\n",
       "        line-height:1.5em;\n",
       "        padding-left:3em;\n",
       "        padding-right:3em;\n",
       "        }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\" <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@600&family=Noto+Sans+JP&display=swap\" rel=\"stylesheet\"> \n",
    "<style>\n",
    "    div.text_cell_render h1 {\n",
    "        font-family: 'Inter';\n",
    "        font-size: 1.7em;\n",
    "        line-height:1.4em;\n",
    "        text-align:center;\n",
    "        }\n",
    "\n",
    "    div.text_cell_render { \n",
    "        font-family: 'Noto Sans JP';\n",
    "        font-size:1.05em;\n",
    "        line-height:1.5em;\n",
    "        padding-left:3em;\n",
    "        padding-right:3em;\n",
    "        }\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Web Data\n",
    "\n",
    "In the previous notebook we identified webpages that host a specific image. The URLs that refer to these webpages are found in .json files. In this notebook, we use the URLs to download the actual pages, extract their textual content and prepare the textual data for analysis. Also, we extract general features of the webpages (the metadata) for analysis. To get an idea of the .json files, we start by opening them and inspecting the list of URLs. \n",
    "\n",
    "First, we define some basic variables. Then we gather the list of .json files and open the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/react-data/npg/npg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-48c3eee65d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Check how many iterations we have by using the os.listdir function. We don't want the \"source\" folder because it doesn't contain jsons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoto_folder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoto_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"source\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mstart_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrange_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/react-data/npg/npg'"
     ]
    }
   ],
   "source": [
    "import os,json\n",
    "\n",
    "base_path = \"D:/react-data/npg\" \n",
    "photo = \"npg\"\n",
    "photo_folder = os.path.join(base_path, photo)\n",
    "\n",
    "# Check how many iterations we have by using the os.listdir function. We don't want the \"source\" folder because it doesn't contain jsons\n",
    "num_iterations = len([fol for fol in os.listdir(photo_folder) if os.path.isdir(os.path.join(photo_folder,fol)) and \"source\" not in fol])\n",
    "start_iter = 1\n",
    "range_iter = [str(i) for i in list(range(1,num_iterations+1))]\n",
    "\n",
    "list_jsons = []\n",
    "\n",
    "# We now \"loop\" through the folders associated with the iterations and gather the .jsons in these folders\n",
    "for iteration in range_iter:\n",
    "    iteration_folder = os.path.join(photo_folder, photo + \"_\" + str(iteration))\n",
    "    list_json_in_iteration_folder = [os.path.join(iteration_folder,js) for js in os.listdir(iteration_folder) if \".json\" in js]\n",
    "    list_jsons += list_json_in_iteration_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can open the .json files by loading them with the json module that comes automatically with your Python installation. You can inspect or \"walk\" the data by selecting keys with names (```json_data['responses']```) or elements in lists (```json_data['responses'][0:10]```). To find the URLs, navigate to the ```pagesWithMatchingImages``` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pageTitle': '&#39;The Girl in the Picture&#39; from Vietnam visits Philly this week for two ...',\n",
       "  'partialMatchingImages': [{'url': 'https://www.inquirer.com/resizer/iytjLFruXq6zbI7pVdDfZrXTDNU=/1400x932/smart/arc-anglerfish-arc2-prod-pmn.s3.amazonaws.com/public/X3VW2BH4ZBFBBMOJCHTLJL52B4.jpg'}],\n",
       "  'url': 'https://www.inquirer.com/arts/girl-in-vietnam-picture-kim-phuc-hannibal-locumbe-20191205.html'},\n",
       " {'pageTitle': 'The &#39;Napalm Girl&#39; To Share Story Of Hope During Free Event ...',\n",
       "  'partialMatchingImages': [{'url': 'https://wpr-public.s3.amazonaws.com/wprorg/styles/facebook/s3/field/image/ap_431676074992.jpg?itok=3jjMa-gm'}],\n",
       "  'url': 'https://www.wpr.org/napalm-girl-share-story-hope-during-free-event-saturday'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(list_jsons[0],'r') as fr:\n",
    "    json_data = json.load(fr)\n",
    "\n",
    "# Show the first elements in the list:\n",
    "json_data['responses'][0]['webDetection']['pagesWithMatchingImages'][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the json data consists of key:value pairs. In the 'pagesWithMatchingImages' list you can find:\n",
    "1. The title of the page where the image is found\n",
    "2. The link to the image file (www.example.com/media/examplephoto.png). This can be either a 'partialMatchingImage' or a 'fullMatchingImage'. The difference between the two is hard to explain, but in most cases a \"fullMatch\" concerns a copy of the input image, only different in scale or quality. A 'partialMatch' usually consists of for example images where the input image is only part of (for example an image of a t-shirt that includes a print of the input image.\n",
    "3. The link to the page itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Pipeline\n",
    "\n",
    "To work with the metadata and text data associated with the webpages we need to extract, clean and harmonize this data. We do so by:\n",
    "- downloading the webpages in .html format\n",
    "- extract the text from the .html pages\n",
    "- identify languages\n",
    "- identify dates\n",
    "- identify Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_folder = os.path.join(base_path, photo)\n",
    "num_iterations = len([fol for fol in os.listdir(photo_folder) if os.path.isdir(os.path.join(photo_folder,fol)) and \"source\" not in fol])\n",
    "start_iter = 1\n",
    "range_iter = [str(i) for i in list(range(1,num_iterations+1))]\n",
    "folder_base = os.path.join(base_path,photo,photo)\n",
    "\n",
    "for iteration in range_iter:\n",
    "\n",
    "    jsFiles = Organize.gatherJson(folder_base,iteration)\n",
    "\n",
    "    # Import previously scraped URLs:\n",
    "    scraped_urls = []\n",
    "    for i in range(1,int(iteration)):\n",
    "        try:\n",
    "            with open(os.path.join(folder_base + \"_\" + str(i), \"html\",\"results.txt\"), 'r', encoding='utf-8') as f:\n",
    "                print(\"INFO: importing from {}\".format(os.path.join(folder_base + \"_\" + str(i), \"html\",\"results.txt\")))\n",
    "                lu = f.readlines()\n",
    "                lu = [l.split('|') for l in lu]\n",
    "                lu = [l for l in lu if len(l) == 2]\n",
    "                lu = [l[1].replace('\\n','') for l in lu]\n",
    "                scraped_urls = scraped_urls + lu\n",
    "        except FileNotFoundError:\n",
    "            print(\"INFO: \", os.path.join(folder_base + \"_\" + str(i), \"html\",\"results.txt\"), \"not found\")\n",
    "\n",
    "    #Scrape All Page URLs to 'image[...]/html' folder\n",
    "    destination_path = os.path.join(folder_base + \"_\" + str(iteration), \"html\")\n",
    "\n",
    "    list_urls = list(set([j['url'] for j in jsFiles]))\n",
    "    list_urls = [u for u in list_urls if u not in scraped_urls]\n",
    "    print('INFO: {} urls left after duplicate detection'.format(len(list_urls)))\n",
    "\n",
    "    HTML.PoolScrape(list_urls, destination_path)\n",
    "    print('INFO: scraping .html files iteration {} finished'.format(iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now find a ```html``` folders inside the ```example_photo1_[iteration number]``` folders. The folder structure now looks like this:\n",
    "```\n",
    "+-- photo_folder\n",
    "    +-- example_photo_1_folder\n",
    "        +-- example_photo_1_1\n",
    "            +-- html (location of downloaded webpages)\n",
    "            +-- img (location of images used for gathering webpages)\n",
    "            +-- photo_name_identifier1.json (json files)\n",
    "            +-- photo_name_identifier2.json\n",
    "        +-- example_photo_1_2\n",
    "        +-- example_photo_1_3\n",
    "        +-- example_photo_1_folder_source\n",
    "```\n",
    "\n",
    "Next, we download the text to a ```txt``` folder that is going to be located at the same level as ```html``` and ```img```. In this folder, we gather all the webpage texts in one single .json file (for easy loading during the analysis). \n",
    "\n",
    "Extracting text from webpages is everything but a straightforward process, because it is not clear beforehand what is relevant. For example, text from ads or menubars are not relevant to the research. Luckily, there is software available that \"parses\" relevant texts. In this pipeline, we use a Python implementation of [boilerpipe](https://github.com/misja/python-boilerpipe) a piece of software that removes clutter and irrelevant bits from webpage texts. Boilerpipe offers various options. Inside the ```function.py``` you will find the setup of our use of the parser. To make things easier we handle the parsing in a separate function, to be called over the different iteration folders. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range_iter:\n",
    "    ParseText.Parse(os.path.join(folder_base+ \"_\" + str(iteration), \"html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the URL text and the webpage text, we can now identifiy the language of the webpage. Here, we make use of ```langid``` a language identification library for Python. Below we identify the languages in a similar way as the we extracted the texts: by iterating over the folders. We write the languages to a .json file that is located in the main photo folder. The language identifier returns a probability score and the best guess language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_folder = os.path.join(base_path, photo)\n",
    "num_iterations = len([fol for fol in os.listdir(photo_folder) if os.path.isdir(os.path.join(photo_folder,fol)) and \"source\" not in fol])\n",
    "start_iter = 1\n",
    "range_iter = [str(i) for i in list(range(1,num_iterations+1))]\n",
    "folder_base = os.path.join(base_path,photo,photo)\n",
    "\n",
    "language_dict = dict()\n",
    "for iteration in tqdm(range_iter):\n",
    "    language_dict.update({str(iteration):dict()})\n",
    "    list_json= [js for js in os.listdir(os.path.join(base_path,photo,photo + \"_\" + str(iteration),\"txt\")) if \".json\" in js]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    if len(list_json) > 0:\n",
    "        for js in list_json:\n",
    "            with open(os.path.join(base_path,photo,photo + \"_\" + str(iteration),\"txt\", js)) as f:\n",
    "                d_ = json.load(f)\n",
    "            val = [\" \".join(i) for i in d_.values()]\n",
    "            ids = [i for i in d_.keys()]\n",
    "            val = pd.DataFrame([ids,val]).T\n",
    "            val.columns = ['id','text']\n",
    "            df = df.append(val)\n",
    "    else:\n",
    "        print('no .json files found')\n",
    "    df['url'] = [i.split('.html_')[1] for i in df['id']]\n",
    "\n",
    "\n",
    "    for c,url in enumerate(df['url']):\n",
    "        language_score = Language.ParseUrl(url)\n",
    "        if language_score is None or language_score[1] < 0.7:\n",
    "            try:\n",
    "                language_score = Language.ParseText(str(df['text'][c])[1:-1])\n",
    "                language_score.append('text')\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        else:\n",
    "            language_score.append('url')\n",
    "        language_dict[str(iteration)].update({url:[language_score[0],language_score[1],language_score[2]]})\n",
    "\n",
    "# Write Detected Languages to language.json\n",
    "with open(os.path.join(base_path,photo,'languages-{}.json'.format(photo)), 'w') as fp:\n",
    "    json.dump(language_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the languages written to a .json, we now repeat a similarly looking procedure for the dates. The ```htmldate``` module identifies the date of publication for a URL. Because the module depends on the avaiability of information embedded in the html, it does not cover all the URLs, but enough to get an idea of the temporal distribution of our data. We write the extracted dates to ```dates.json``` in the main photo folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_urls = dict()\n",
    "\n",
    "for iteration in range_iter:\n",
    "    try:\n",
    "        with open(os.path.join(base_path, photo, photo + \"_\" + str(iteration), \"html\", \"results.txt\"), 'r', encoding='utf-8') as f:\n",
    "            lu = f.readlines()\n",
    "        lu = [l.split('|') for l in lu]\n",
    "        lu = [l for l in lu if len(l) == 2]\n",
    "        lu = [l[1].replace('\\n','') for l in lu]\n",
    "        print(\"---- {} dates found in iteration {}\".format(len(lu),iteration))\n",
    "        scraped_urls.update({str(iteration):lu})\n",
    "    except Exception as e:\n",
    "        print(\"Error: \",e)\n",
    "\n",
    "dates_dict = dict()\n",
    "for it,list_ in scraped_urls.items():\n",
    "    dates_dict.update({str(it):dict()})\n",
    "    print('---- Scraping Dates Iteration {}, {} URLs'.format(it,len(list_)))\n",
    "    if sampling == True:\n",
    "        print('----- Sampling with Size {}'.format(sample_size))\n",
    "        if len(list_) < sample_size:\n",
    "            list_ =list_\n",
    "        else:\n",
    "            list_ = random.sample(list_,sample_size)\n",
    "\n",
    "    for u in tqdm(list_):\n",
    "        try:\n",
    "            date = WebPage.gatherSingleDate(u)\n",
    "            dates_dict[str(it)].update({u:date})\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "with open(os.path.join(base_path,photo,'dates-{}.json'.format(photo)), 'w') as fp:\n",
    "    json.dump(dates_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we identify Named Entities in our text data. This method extracts entities (locations, persons, dates etc.) from the text, based on pretrained models. We use the popular Spacy models (available for english, dutch, italian, french, spanish, portugese) to do this. Named Entities can be used to study the context of certain keywords, and the phenomena associated with the photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F. Named Entitiy Recognition using Spacy\n",
    "print(\"INFO: Language Detection & Named Entitiy Recognition using Spacy\")\n",
    "\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "selected_languages = \"en de fr es it nl pt\".split(' ')\n",
    "selected_languages = {i:i+\"_core_news_sm\" for i in selected_languages}\n",
    "selected_languages.update({\"en\":\"en_core_web_sm\"})\n",
    "\n",
    "def PreProc(text):\n",
    "    text = text[1:-1].replace('\\xa0', ' ')\n",
    "    text = \" \".join(text.split('\\r\\n'))\n",
    "    return text\n",
    "\n",
    "d_ = dict()\n",
    "for iteration in range_iter:\n",
    "\n",
    "    # Language Detection\n",
    "    list_csv = [csv for csv in os.listdir(os.path.join(base_path, photo, photo + \"_\" + str(iteration),\"txt\")) if \".csv\" in csv]\n",
    "\n",
    "    df= pd.DataFrame()\n",
    "    for csv in list_csv:\n",
    "        tmp = pd.read_csv(os.path.join(base_path, photo, photo + \"_\" + str(iteration),\"txt\",csv))\n",
    "        df = df.append(tmp)\n",
    "\n",
    "    df['text'] = [PreProc(str(i)) for i in df['text']]\n",
    "    df['lang'] = [identifier.classify(i)[0] for i in df['text']]\n",
    "    df.to_csv(os.path.join(base_path, photo, \"text-language-{}.csv\".format(photo)),index=False)\n",
    "\n",
    "    # NER\n",
    "    for lang in [i for i in list(set(df['lang'])) if i in selected_languages.keys()]:\n",
    "        if lang not in d_.keys():\n",
    "            d_.update({lang:dict()})\n",
    "        nlp = spacy.load(selected_languages[lang])\n",
    "        tmp = df[df['lang'] == lang]\n",
    "\n",
    "        for count,text in enumerate(df['text']):\n",
    "            identif = str(df['id'][count])\n",
    "            d_[lang].update({identif:dict()})\n",
    "            d_[lang][identif].update({\"text\":text})\n",
    "            doc = nlp(text)\n",
    "            doc = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "            d_[lang][identif].update({\"entities\":doc})\n",
    "\n",
    "with open(os.path.join(base_path, photo,\"entities-{}.csv\".format(photo)), 'w') as fp:\n",
    "    json.dump(language_dict, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
