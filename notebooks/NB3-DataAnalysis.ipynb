{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\" <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@600&family=Noto+Sans+JP&display=swap\" rel=\"stylesheet\"> \n",
    "<style>\n",
    "    div.text_cell_render h1 {\n",
    "        font-family: 'Inter';\n",
    "        font-size: 1.7em;\n",
    "        line-height:1.4em;\n",
    "        text-align:center;\n",
    "        }\n",
    "\n",
    "    div.text_cell_render { \n",
    "        font-family: 'Noto Sans JP';\n",
    "        font-size:0.8em;\n",
    "        line-height:1.5em;\n",
    "        padding-left:3em;\n",
    "        padding-right:3em;\n",
    "        }\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Data\n",
    "\n",
    "After downloading and structuring the data in the previous notebooks, this notebook discusses the analysis of the data. First, metadata analysis is explained. This entails the aggregation and visualization of:\n",
    "- the number of webpages through time\n",
    "- the most frequent top level domains (through time)\n",
    "- the distribution of languages (through time)\n",
    "\n",
    "After working with the metadata, the notebook explains three ways of analyzing the textual content of the pages. This includes the aggregation and visualization of:\n",
    "- the most frequent words found on the webpages (hereby differentiating between languages)\n",
    "- the most topical words for every document, through the TFIDF statistic\n",
    "- the words that co-occur frequently with a keyword, through the PMI statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering the Data\n",
    "In the previous notebooks, various types of data were downloaded and parsed. The webpages are in the ```html``` folders, the dates and languages are stored in ```.json``` files, similar to the webpage text. Analyzing the data requires the combination of data types. For that reason, it is useful to gather the data in one place. Below the data stored in different locations is imported into Python. Subsequently, it is combined in one, single \"dataframe\". A dataframe is a Python object that contains data. It is comparable to an Excel table. In fact, dataframes can be exported to Excel or .csv (comma-separated) files. Moreover, dataframes are easy to query and visualizing the data stored in dataframes is easy to do with the popular dataframe library Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from purl import URL\n",
    "import pandas as pd\n",
    "import langid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/path/to/ReACT_GCV/notebooks/photo_folder\"\n",
    "photo = \"9\"\n",
    "photo_folder = os.path.join(base_path, photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the dates from the dates-[photo].json file in the photo folder. Dates are processed per iteration, \n",
    "so first access the iterations one by one, and then the url:date pairs in those iterations.\n",
    "\n",
    "Note that not all URLs have associated dates. We filter them out by removing URLs with either \"na\" or \"ERROR\"\n",
    "as their value.\n",
    "'''\n",
    "dates_dictionary = dict()\n",
    "\n",
    "with open(os.path.join(photo_folder,\"dates-{}.json\".format(photo)),'r') as fp:\n",
    "    x = json.load(fp)\n",
    "    \n",
    "for iteration, pairs in x.items():\n",
    "    \n",
    "    for url, date in pairs.items():\n",
    "        if date != \"na\" or \"ERROR\" not in date:\n",
    "            dates_dictionary.update({url:date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import that languages from the languages-photo.json file. Here we do a similar thing: import the .json file and store the identifier - language pairs\n",
    "in a dictionary. \n",
    "'''\n",
    "language_dictionary = dict()\n",
    "\n",
    "photo_folder = os.path.join(base_path, photo)\n",
    "\n",
    "with open(os.path.join(photo_folder,'languages-'+photo+\".json\"),'r') as f:\n",
    "    lang = json.load(f)\n",
    "\n",
    "languages = []\n",
    "\n",
    "for iterkey,items1 in lang.items():\n",
    "\n",
    "    for id_, lan_items in items1.items():\n",
    "        language = lan_items[0] # the language identifier also outputs the probability that the guess is right. We don't need that so we discard it here\n",
    "        language_dictionary.update({id_:language})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import texts from parsed_text.json file in iteration folders. N.B.: this means that for every iteration one .json with texts is constructed.\n",
    "Hence the extra loop. This loop also combines all the information in one object. In this process we also extract the so-called Top Level\n",
    "Domain (TLD), for example www.facebook.com in the case of the URL www.facebook.com/user/something. \n",
    "'''\n",
    "\n",
    "text_dictionary = dict()\n",
    "\n",
    "photo_folder = os.path.join(base_path, photo)\n",
    "num_iterations = [fol for fol in os.listdir(photo_folder) if os.path.isdir(os.path.join(photo_folder,fol)) and \"source\" not in fol and \"context\" not in fol]\n",
    "num_iterations = len(num_iterations)\n",
    "\n",
    "start_iter = 1\n",
    "range_iter = [str(i) for i in list(range(1,num_iterations + 1))]\n",
    "\n",
    "folder_base = os.path.join(base_path,photo,photo)\n",
    "\n",
    "for iteration in range_iter:\n",
    "    fn = os.path.join(folder_base + \"_\" +str(iteration),\"txt\", \"parsed_text.json\")\n",
    "    with open(fn) as fp:\n",
    "        pages = json.load(fp)\n",
    "\n",
    "    for identifier,sentences in pages.items():\n",
    "        \n",
    "        sentences = sentences.replace(\"\\n\",\"\")\n",
    "        sentences = re.sub(' +', ' ', sentences)\n",
    "\n",
    "        url = identifier.split('html_')[-1]\n",
    "        if url in dates_dictionary.keys():\n",
    "            date = dates_dictionary[url]\n",
    "        else:\n",
    "            date = \"na\"\n",
    "\n",
    "        text_dictionary.update({identifier:dict()})\n",
    "        text_dictionary[identifier].update({\"photo\":photo,\"url\":url,\"identifier\":identifier,\"date\":date,\"language\":language_dictionary[identifier],\"sentences\":sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we transform the constructed dictionary is a DataFrame that can be exported to a .csv file. To be sure that there are no duplicate pages in our dataset\n",
    "we remove the duplicates.\n",
    "'''\n",
    "\n",
    "df = pd.DataFrame.from_dict(text_dictionary,orient='index').reset_index()\n",
    "df = df.drop_duplicates('url',keep='first')\n",
    "df.columns = [\"path\",\"photo\",\"url\",\"identifier\",\"date\",\"language\",\"sentences\"]\n",
    "#df.to_csv('path/to/datafolder/data-full.csv',index=False)\n",
    "data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Part 1. Diachronic Frequency\n",
    "\n",
    "We now have a dataframe where every row represents a webpage. The columns contain information about the path to the webpage text, the name of the photo, the URL, the identifier, the date, the language and the actual text. \n",
    "\n",
    "With the data stored in such a format, it is relatively straightforward to study some basic frequential patterns (although the Pandas dataframe syntax can be a bit overwhelming at first. We start by plotting the number of webpages in every year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('path/to/datafolder/data-full.csv')\n",
    "\n",
    "subset = data[['photo','date']] # get the photo and date columns from the Pandas dataframe stored in the 'data' variable\n",
    "subset['count'] = 1 # add a count column, because every row is one observation (this comes in useful later)\n",
    "\n",
    "# loop over the dates and if the date is not \"na\" or \"nan\": extract the year (the first 4 characters of the date)\n",
    "subset['year'] = ''\n",
    "\n",
    "for c,i in enumerate(subset['date']):\n",
    "    \n",
    "    if \"na\" not in str(i): # \n",
    "        year = str(i)[0:4]\n",
    "        subset['year'][c] = year\n",
    "    else:\n",
    "        subset['year'][c] = \"na\"\n",
    "        \n",
    "# Remove all observations with \"na\"\n",
    "subset = subset[~subset['year'].isin([\"na\",\"nan\"])]\n",
    "\n",
    "# Group by photo and year and add the values\n",
    "subset = subset.groupby(['photo','year']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we have a dataframe that has three columns: the name of the photo, the year and the webpage frequency. Now it is possible to plot the data.\n",
    "If you used only one photo you can simply plot the years against the frequency:\n",
    "'''\n",
    "p = subset.plot.bar(x='year',y='count',figsize=(10,6), color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If you have multiple photos, mutate the dataframe in such a way that it has the following column structure:\n",
    "\n",
    "year | photo_name_1 | photo_name_2\n",
    "1999 | 4            | 55\n",
    "\n",
    "This operation also works if you have one photo, so you can safely run this cell\n",
    "'''\n",
    "\n",
    "subset = subset.pivot(index='year',columns='photo',values='count').reset_index()\n",
    "p = subset.plot.bar(x='year',y=list(subset.columns)[1:],figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Part 2. Top Level Domains\n",
    "\n",
    "The URLs that refer to the pages were the image is found are highly specific. They consist of several compontents: \n",
    "- the \"http\" component that indicates the use of the Hypertext Transfer Protocol, a set of rules for web traffic\n",
    "- the \"www\" component that indicates that the page is on the World Wide Web\n",
    "- the \"top level domain\" (\"facebook\", \"bbc\", \"twitter\") the name of the website\n",
    "- a domain extension (\".nl\",\".com\") \n",
    "- a \"path\" that refers to specific data or pages on the website \n",
    "\n",
    "Especially the top level domains (TLDs) and the extensions can help in identifying frequently occuring websites. To extract the TLDS we use an existing URL-parser, a Python module that identifies the parts of the URL.\n",
    "\n",
    "To extract the TLDs we use something called \"list comprehension\". This is a very effective way of doing things with lists. Imagine the following list: ```[1,2,3,4,5]```, stored in the variable ```list1```. The list elements can be shown by doing ```[item for item in list1]```. Once we know this, we could also do ```[item + 1 for item in list1]```, which results in ```[2,3,4,5,6]```. In fact, we could do anything in this way. Below we use the ```URL.from_string(x).domain()``` function to extract the TLDs.\n",
    "\n",
    "Next, we use a very handy module called ```Counter()``` to get the most frequent TLDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlds = [URL.from_string(u).domain() for u in list(set(data['url']))]\n",
    "Counter(tlds).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Part 3. Languages\n",
    "\n",
    "We already know something of the languages by looking at the extensions. A better way to look at the distribution of languages is identifying the language of the webpage text itself. Luckily, several Python modules are able to identify the language of a text. In the previous notebook, we used ```langid.classify()``` to identify languages. Since it is very fast, we repeat this here. Because the classifier also outputs the probability of a correct guess (which is always very high because we have lots of textual data) we have to select the first element from its output (```identifier.classify('example webpage text')[0]```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "\n",
    "data['language'] = [identifier.classify(x)[0] for x in list(data['sentences'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we use Counter() to find the language distribution\n",
    "'''\n",
    "\n",
    "Counter(list(data['language'])).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "So far we have worked with the metadata. The contents of the webpages are also a valuable source of information. This is where we move into the area of text analysis. In this notebook we will work on the basic unit of text analysis: word counts.\n",
    "\n",
    "Before we can count words, we need to again reformat the data. This involves several steps:\n",
    "- removing the '||' separators we used in the parsing\n",
    "- splitting the web page text into tokens. Tokens are \"an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing\" ([Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)). This often translates into words, but a \".\" or a \"&\" is also identified as a token. For the tokenization of the data we use the powerful ```nltk``` (Natural Language Toolkit).\n",
    "\n",
    "Also, we cannot really compare words in multiple languages, so we first subset a dataset with a language of your choice (in the example it is italian, marked as 'it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_subset = data[data['language']=='it']['sentences']\n",
    "texts_subset = [t.replace('||','') for t in texts_subset]\n",
    "texts_subset = [nltk.word_tokenize(t) for t in texts_subset]\n",
    "\n",
    "# Merge all the webpages into one 'Bag of Words' using a more complex form of list comprehension (that 'flattens' a list of lists)\n",
    "subset_bow = [word for webpage in texts_subset for word in webpage]\n",
    "\n",
    "# find the most frequent words\n",
    "Counter(subset_bow).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words shown above do not come as a surprise. Words like 'the' and 'a' occur numerous times and do not tell us anything about the semantic content of the webpages. One way to come closer to the contents of the pages is to remove frequently occuring 'stopwords'. This can also be done with ```nltk```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "subset_bow_nostops = [word for word in subset_bow if word not in stopwords.words('italian') and len(word) > 2] #replace italian by \"english\" or \"french\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(subset_bow_nostops).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that appear in the list after removing the stopwords are already more indicative of the contents of the webpage. Word frequencies are the building blocks of many other methods. Two of them are particularly useful and will be introduced in the remainder of this notebook.\n",
    "\n",
    "One useful measure is \"Term Frequency Inverted Document Frequency\" (TFIDF). This measure calculates the likelihood that a word appears in a document and multiplies this term frequency (TF) by the Inverted Document Frequency (IDF), that is: (the log of) the number of documents divided by the number of documents that contain a specific word. This can be used to check which words appear in a document, thereby taking into account the frequency of that word in the whole corpus. If the word \"protest\" occurs frequently on one webpage, but not very frequently on other pages, it will have a high TFIDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the unique words, excluding stopwords\n",
    "unique_words = list(set(subset_bow_nostops)) \n",
    "\n",
    "# Create a dataframe\n",
    "term_frequency = []\n",
    "\n",
    "for word in unique_words:\n",
    "    \n",
    "    for document_number,webpage in enumerate(texts_subset): #we already made texts_subset:\n",
    "            word_frequency_document = len([w for w in webpage if w == word]) / len(webpage)\n",
    "            tmp = [document_number,word,word_frequency_document] # make a list of the webpagename, word and the frequency of the word on the page\n",
    "            term_frequency.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "document_frequency = dict()\n",
    "\n",
    "for word in unique_words:\n",
    "    doc_freq = len([webpage for webpage in texts_subset if word in webpage])\n",
    "    inverse_doc_freq = math.log(len(texts_subset) / doc_freq)  \n",
    "    document_frequency.update({word:doc_freq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_list = []\n",
    "\n",
    "for document_number,word,word_frequency_document in term_frequency:\n",
    "    tfidf_score = word_frequency_document * document_frequency[word]\n",
    "    tfidf_list.append([document_number,word,tfidf_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(tfidf_list,columns=['doc','word','tfidf']) # make a dataframe\n",
    "tfidf = tfidf.pivot(index='word',columns='doc',values='tfidf') # transform the \"long\" dataframe to a \"wide\" one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.sort_values(2,ascending=False)[0][0:10] # select document 2, sort the largest values and select column 2 (corresponds with document 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information\n",
    "\n",
    "TFIDF is great for investigating differences between web pages or even clustering them together. However, for many research questions we are actually interested in the 'behaviour' of specific words. One way of investigating single words is collocation analysis. Here, we evaluate which words often co-occur in the same sentence. A formal statistic for collocation analysis is PMI (Pointwise Mutual Information). Here, we calculate the probability of the target word (example: \"protest). This probability is calculated by taking the frequency of \"protest\" in the corpus and sharing that by the number of words in the corpus. Subsequently, we select another word (example: \"violence\"). We calculate the probability that the two words appear together. What \"together\" means is up to you. It is good practice to take the sentence as a measure for togetherness. In other words, we identify the sentences (with ```nltk.sent_tokenize()```), count the number of co-occurrences and share this frequency by the number of sentences. Lastly, we take the log of P(x,y) / P(x) * P(y) in order to normalize the effect of frequency, similar to TFIDF.\n",
    "\n",
    "To put it in mathematical terms: \"The PMI of a pair of outcomes x and y belonging to discrete random variables X and Y quantifies the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions, assuming independence\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency = dict()\n",
    "\n",
    "# Create a dataframe\n",
    "term_frequency = []\n",
    "\n",
    "for word in unique_words:\n",
    "    \n",
    "    for document_number,webpage in enumerate(texts_subset): #we already made texts_subset:\n",
    "            word_frequency_document = len([w for w in webpage if w == word]) / len(webpage)\n",
    "            tmp = [document_number,word,word_frequency_document] # make a list of the webpagename, word and the frequency of the word on the page\n",
    "            term_frequency.append(tmp)\n",
    "\n",
    "sentences = [nltk.sent_tokenize(d) for d in all_docs]\n",
    "sentences = [item for sublist in sentences for item in sublist]\n",
    "print(\"Number of Sentences: {}\".format(len(sentences)))\n",
    "\n",
    "# Get PMI values\n",
    "for kw in keywords:\n",
    "    pmi_d = []\n",
    "    p_keyword = len([s for s in sentences if kw in s.split(' ')]) / len(sentences) * 100\n",
    "\n",
    "    for w in target_word_dict[kw]:\n",
    "        ntarget = len([s for s in sentences if w in s.split(' ')])\n",
    "        nxy = len([s for s in sentences if w in s and kw in s.split(' ')])\n",
    "\n",
    "        if ntarget < threshold or nxy < threshold:\n",
    "            continue\n",
    "\n",
    "        ptarget = len([s for s in sentences if w in s]) / len(sentences) * 100\n",
    "        pxy = len([s for s in sentences if w in s and kw in s]) / len(sentences) * 100\n",
    "\n",
    "\n",
    "        if len([s for s in sentences if w in s and kw in s]) > 0:\n",
    "            pmi = math.log((pxy) / (ptarget * p_keyword))\n",
    "            pmi_d.append([w,pmi,ntarget,nxy])\n",
    "        else:\n",
    "            continue\n",
    "    if len(pmi_d) > 0:\n",
    "        t = pd.DataFrame(pmi_d,columns = ['w','s','ntarget','ntogether'])\n",
    "        t.to_csv('/media/ruben/FEF44259F44213F5/Users/Ruben/Documents/GitHub/ReACT_GCV/data/images_tables_article_carlo/pmi-{}-en.csv'.format(kw),index=False)\n",
    "    else:\n",
    "        print(\"{} has no PMI candidates that meet the threshold\".format(kw))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
