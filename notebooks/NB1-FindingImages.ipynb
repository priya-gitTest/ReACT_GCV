{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\" <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@600&family=Noto+Sans+JP&display=swap\" rel=\"stylesheet\"> \n",
    "<style>\n",
    "    div.text_cell_render h1 {\n",
    "        font-family: 'Inter';\n",
    "        font-size: 1.7em;\n",
    "        line-height:1.4em;\n",
    "        text-align:center;\n",
    "        }\n",
    "\n",
    "    div.text_cell_render { \n",
    "        font-family: 'Noto Sans JP';\n",
    "        font-size:1.05em;\n",
    "        line-height:1.5em;\n",
    "        padding-left:3em;\n",
    "        padding-right:3em;\n",
    "        }\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Google Vision API\n",
    "\n",
    "This notebook describes the process of interacting with the Google Cloud Vision API (Application Programming Interface). The API can be used for a multitude of tasks and methods: from identifying objects on images to extracting text from images. One feature, the so-called \"web detection\" module is particularly usefull for humanities research. This feature of the API allows the identification of replications of a specific image in the Google index.** The module returns a list of web adresses (URLs) that can be used to gather information about the afterlives of images. This notebook explains how use the API.\n",
    "\n",
    "The interaction with the API will be explained in this Jupyter Notebook. A Notebook is a more interactive and visually comprehensible Python script. A Python script is usually a \".py\" file that contains some code. If you have installed Python on your system you can run these scripts in a terminal. A python script runs at once. A Jupyter Notebook, however, is a python script that does not run at once, but in cells. You can execute every cell separately. This allows for a step-by-step execution of the script, and, for example, the inspection of the data you work with. \n",
    "\n",
    "Cells are executed by pressing ```Shift``` + ```Enter```.\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "** Google's index of the Internet is not the same as the Internet itself but only a finegrained \"roadmap\". The possibility exists that webpages that host an image are not indexed by Google since it is estimated that Google indexed only [4% of all existing webpages](https://eu.tennessean.com/story/money/tech/2014/05/02/jj-rosen-popular-search-engines-skim-surface/8636081/) (this still concerns several trillion pages). Moreover, websites that existed in the past but are no longer present on the web fall outside the scope of the API. These limitations must be kept in mind when using the API and analyzing its output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an API?\n",
    "\n",
    "An Application Programming Interface (API) is a way to connect to a service or dataset. In the case of Google Cloud Vision, the API communicates between the Google service (which runs on a server somewhere) and your machine. An API can be compared with a \"save as\" button or a search interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your Google Cloud Account\n",
    "\n",
    "Before we begin the interaction, we need to set up our account at Google. All the steps are described [here](https://cloud.google.com/vision/docs/before-you-begin), but we will walk through them in this notebook as well.\n",
    "\n",
    "1. Make a Google Account (or use an existing one). You can register [here](https://accounts.google.com/signup/v2/webcreateaccount?service=mail&continue=https%3A%2F%2Fmail.google.com%2Fmail%2F&ltmpl=default&dsh=S-1022536791%3A1581929757665542&gmb=exp&biz=false&flowName=GlifWebSignIn&flowEntry=SignUp). It is advised to make a new Google account.\n",
    "\n",
    "2. Google has a broad range of API services. We use only the Cloud Vision API. Activate it [here](https://console.cloud.google.com/flows/enableapi?apiid=vision.googleapis.com&_ga=2.55858594.803327971.1581929623-746618159.1580742281). \n",
    "\n",
    "3. Once we have activated the API, it requires authentication. This means that we need to create a project that comes with files that _identify_ that specific project. To do this, create a \"service account key\" [here](https://console.cloud.google.com/apis/credentials/serviceaccountkey?_ga=2.52048288.803327971.1581929623-746618159.1580742281). Select \"new service account\" and enter a name for your project. Then, select the role of \"owner\" for yourself in the project. Lastly, click on \"create\". This will prompt the download of a .json file. JSON is a popular file type for storing information. In this .json file your project credentials are found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction with the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction with the API is done by creating URLs that request information, similar to typing URLs in the address bar. Creating requests can be done in many ways. Google has a nice explanation of making an API request from scratch [here](https://cloud.google.com/vision/docs/internet-detection).\n",
    "\n",
    "Because building an API key from scratch is a relatively complex task, we created a set of Python scripts that handle most of the coding. These scripts are found in the ```scrapelib``` folder. They can be loaded into this notebook by placing the placing the ```scrapelib``` folder and the ```gcv_api.py``` and ```functions.py``` files in the same folder as this notebook. Subsequently we import them, along with some other libraries. \n",
    "\n",
    "This immediately touches upon one of the core elements of Python: modules. Modules are secondary sets of Python scripts that handle specific tasks. There is for example a module for making visualizations, and a module for working with images. The ```scrapelib``` folder, together with the ```gcv_api.py``` and ```functions.py``` files can be seen as a module for working with the API. Additionally, some other modules need to be loaded. We do so by literally importing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from gcv_api import main\n",
    "from functions import *\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to run some code. When working with large datasets, it's important to have a proper folder structure. In this case, the code needs the following structure:\n",
    "\n",
    "```\n",
    "+-- photo_folder\n",
    "|   +-- example_photo_1_folder\n",
    "        +-- example_photo_1_folder_source\n",
    "            +-- example_photo_1.jpg\n",
    "|   +-- example_photo_2_folder\n",
    "        +-- example_photo_2_folder_source\n",
    "            +-- example_photo_2.jpg\n",
    "```\n",
    "\n",
    "The scripts going to look at all the photo folders in the main folder. In every subfolder the original image file needs to be located in the ```source``` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides having a folder structure, we need to define some variables. We define:\n",
    "- the API key from your own account (usually a very long code, something like 912421-09821f-n13r39)\n",
    "- the path to the folder were all the photos are stored (```photo_folder``` in the example) \n",
    "- the name of the photo we want to scrape (```example_photo_1.jpg```). \n",
    "- the input folder (a combination of the top ```photo_folder```, the name of the photo and the ```source``` folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyCZFu_wHsXURDLgSDlUuUAwDnGwKrgBKUc\"\n",
    "base_path = \"media/ruben/FEF44259F44213F5/Users/Ruben/Documents/GitHub/ReACT_GCV/notebooks/photo_folder\" #set path to notebooks/photo_folder from the C drive, or /home\n",
    "photo = \"example_photo_1_folder\"\n",
    "input_folder_ = os.path.join(base_path,photo,photo + \"_\" + \"source\")\n",
    "print(\"Image in input folder: {}\".format(os.listdir(input_folder_)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our input folder, it's time to call the API! As said, this only requires feeding some variables into the ```scrapelib``` library. Above we have called that library from the ```gcv_api.py``` file placed in the same folder as this notebook. If you want to know more about the inner workings of the scraper function, just take a look at the Python scripts in the ```scrapelib``` folder. \n",
    "\n",
    "Below we call the API using the ```main.main``` function.  We use the variables we have just set. For now, we use the variables we just defined and set ```iteration``` to ```1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.main(\n",
    "            input_folder = input_folder_,\n",
    "            key = api_key,\n",
    "            output_folder = os.path.join(base_path, photo, photo + \"_\"),\n",
    "            iteration = 1\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this function, you will find a .json file in the output folder. JSON files are very helpful in ordering data. They are structured hierarchically, which means that they contain identifier/values pairs. One of the identifiers is ```pagesWithMatchingImages```. It is under this identifier that we find a list of URLs. These are the pages where your image is located.\n",
    "\n",
    "We will discuss how to handle these files in the next notebooks. In the remainder of this notebook, we will focus on something else. When your image is famous or iconic, it is likely that there are hundreds, thousands or even millions of associated webpages. The Google API seems to have an internal limit on the number of webpages it returns in one .json file. This is problematic, because we do not know which URLs are returned and which ones are omitted. In fact, it seems that Google just returns the most recent URLs that host your image. \n",
    "\n",
    "To solve this (i.e. extent the number of results) we make use of a rather simple trick: we feed the images associated with the URLs found in the [output].json file back into the API. Because the images found by the API are likely to contain small variations: different colortones, dimensions or sizes. Because of this variation, the API tracks new instances of similar images. We can repeat this trick until we encounter no more new URLs.\n",
    "\n",
    "This method has some implications. First, in theory we are dealing with exponential growth in the number of results. Assuming that each scraped image returns about 100 URLs and that the images present on those webpages are uploaded, we end up with 100 x 100 x 100 x 100 x 100 = 10 billion results after five iterations. Luckily, this is only theory because every iteration will yield many duplicate URLs. It is likely that some URLs reappear after some iterations. This is the reason that we have to remove the duplicates, also because you will burn your free Google Cloud credits fast if you just upload the same image over and over.\n",
    "\n",
    "Below we upload the results from the first iteration \"back\" into the API. To do so, we need to gather all the web addresses of the images. These are also included in the .json files. To make things easier, we included a function to gather them in the ```functions.py``` file. We indicate the iteration number in the variable n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_json = [os.path.join(base_path, photo, photo + \"_\" + str(n),f) for f in os.listdir(os.path.join(base_path,photo,photo + \"_\" + str(n))) if '.json' in f]\n",
    "print('looking for scraped image URLs in {}'.format(list_json))\n",
    "image_url_current = Json.extract_image_folder(list_json)\n",
    "[print(x) for x in image_url_current[0:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the URLs referring to the actual images in the variable ```image_url_current```. After we check if the list is longer than 1 (otherwise we don't have anything to download) we download the images to our own computer. The previously defined variables are used to create a new folder called ```img``` in the folder belonging to the nth iteration. Because scraping uses some additional libraries that are a bit complex, you can call the scraping function ```Img.scrape``` to your list of URLs. Don't forget to set the so-called environment variable to the destination folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are Images to Scrape, if not: break and go to next Photo\n",
    "if len(image_url_current) == 0 or image_url_current is None:\n",
    "    print(\"No URLs found in Iteration {}, going to next photo\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape images\n",
    "images_destination = os.path.join(base_path,photo,photo + \"_\" + str(n), \"img\")\n",
    "if not os.path.exists(images_destination):\n",
    "    os.makedirs(images_destination)\n",
    "\n",
    "os.chdir(images_destination)\n",
    "Img.Scrape(image_url_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading all the images, there is one thing left to do: remove the duplicates based on the URLs in the JSON. In subsequent iterations of scraping we do this in advance (remove duplicate URLs from the list). Additionally, we remove images that are very small. The threshold size is currently 4000 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "if len(image_url_current) > 1:\n",
    "    Img.RemoveSmall(images_destination,4000)\n",
    "    Duplicates.remove(images_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline\n",
    "\n",
    "That's it! You now have the code to find images on the internet. To search for images beyond the first iteration, use the cell below and adjust the iteration number (in this case we start with 2 because we did the first iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "\n",
    "# Post Images to API\n",
    "if int(n) == 1:\n",
    "    input_folder_ = os.path.join(base_path, photo, photo + \"_\" + \"source\")\n",
    "\n",
    "if int(n) > 1:\n",
    "    input_folder_ = os.path.join(base_path, photo, photo + \"_\" + str(int(n)-1), \"img\")\n",
    "\n",
    "try:\n",
    "    main.main(\n",
    "            input_folder = input_folder_,\n",
    "            key = api_key,\n",
    "            output_folder = os.path.join(base_path, photo, photo + \"_\"),\n",
    "            iteration = n\n",
    "            )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Gather Image URLs from Output files (.json files) and (if n > 1) remove duplicates\n",
    "list_json = [os.path.join(base_path, photo, photo + \"_\" + str(n),f) for f in os.listdir(os.path.join(base_path,photo,photo + \"_\" + str(n))) if '.json' in f]\n",
    "print('looking for scraped URLs in {}'.format(list_json))\n",
    "image_url_current = Json.extract_image_folder(list_json)\n",
    "\n",
    "if int(n) > 1:\n",
    "    processed_urls = []\n",
    "    for iter_previous in range(1,int(n)):\n",
    "        list_json_prev = [os.path.join(base_path, photo, photo + \"_\" +str(iter_previous),f) for f in os.listdir(os.path.join(base_path,photo,photo + \"_\" + str(iter_previous))) if '.json' in f]\n",
    "        print('looking for previous URLs in {}'.format(list_json_prev))\n",
    "        image_url = Json.extract_image_folder(list_json_prev)\n",
    "        processed_urls = processed_urls + image_url\n",
    "\n",
    "    duplicates = [u for u in image_url_current if u in processed_urls]\n",
    "    print(\"{}/{} image-URLs removed (duplicates)\".format(len(duplicates),len(image_url_current)))\n",
    "    image_url_current = [u for u in image_url_current if u not in list(set(processed_urls))]\n",
    "\n",
    "# Check if there are Images to Scrape, if not: break and go to next Photo\n",
    "if len(image_url_current) == 0 or image_url_current is None:\n",
    "    print(\"No URLs found in Iteration {}, going to next photo\".format(n))\n",
    "    exit()\n",
    "\n",
    "# Scrape images\n",
    "images_destination = os.path.join(base_path,photo,photo + \"_\" + str(n), \"img\")\n",
    "if not os.path.exists(images_destination):\n",
    "    os.makedirs(images_destination)\n",
    "\n",
    "os.chdir(images_destination)\n",
    "Img.Scrape(image_url_current)\n",
    "\n",
    "# Remove duplicates\n",
    "if len(image_url_current) > 1:\n",
    "    Img.RemoveSmall(images_destination,4000)\n",
    "    Duplicates.remove(images_destination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
