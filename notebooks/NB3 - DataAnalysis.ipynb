{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@600&family=Noto+Sans+JP&display=swap\" rel=\"stylesheet\"> \n",
       "<style>\n",
       "    div.text_cell_render h1 {\n",
       "        font-family: 'Inter';\n",
       "        font-size: 1.7em;\n",
       "        line-height:1.4em;\n",
       "        text-align:center;\n",
       "        }\n",
       "\n",
       "    div.text_cell_render { \n",
       "        font-family: 'Noto Sans JP';\n",
       "        font-size:1.05em;\n",
       "        line-height:1.5em;\n",
       "        padding-left:3em;\n",
       "        padding-right:3em;\n",
       "        }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\" <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@600&family=Noto+Sans+JP&display=swap\" rel=\"stylesheet\"> \n",
    "<style>\n",
    "    div.text_cell_render h1 {\n",
    "        font-family: 'Inter';\n",
    "        font-size: 1.7em;\n",
    "        line-height:1.4em;\n",
    "        text-align:center;\n",
    "        }\n",
    "\n",
    "    div.text_cell_render { \n",
    "        font-family: 'Noto Sans JP';\n",
    "        font-size:1.05em;\n",
    "        line-height:1.5em;\n",
    "        padding-left:3em;\n",
    "        padding-right:3em;\n",
    "        }\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Data\n",
    "\n",
    "The webpages associated with the input image(s) are now downloaded and enriched with metadata. Now we can start analyzing the data. In this notebook we analyze several features of the data: \n",
    "- the diachronic frequency (when was the image published)\n",
    "- the top level domains (on what websites was the image found)\n",
    "- the distribution of languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformatting the Data\n",
    "Before we do any (text) analysis, it is useful to assemble the data and put it all in one file. Below we pull together the information found in the .json files with dates, texts and entities. We reformat the information into one .csv (comma-separated) file. CSV-files can be opened in for example Excel and Notepad and are easy to use in for example Excel or Notepad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/media/ruben/Data Drive/react-data/protest/carlo-batch-selection\"\n",
    "\n",
    "'''\n",
    "Import the dates from the dates.txt file in the photo folder. The dates.txt file contains the URL and the associated date, separated by '||'.\n",
    "We create a dictionary with the URL-date pair. This requires splitting the line on the '||' characters and excluding the URLs that have no date\n",
    "associated (they are marked as 'na')\n",
    "'''\n",
    "dates_ref = dict()\n",
    "\n",
    "for photo in [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]:\n",
    "    photo_folder = os.path.join(base_path, photo)\n",
    "    with open(os.path.join(photo_folder,\"dates.txt\"),'r') as f:\n",
    "        x = f.readlines()\n",
    "    dates_ref.update({d.split('|')[0]:d.split('|')[-1].replace('\\n','') for d in x if d.split('|')[-1].replace('\\n','') != \"na\" and \"ERROR\" not in d.split('|')[-1].replace('\\n','')})\n",
    "\n",
    "'''\n",
    "Import that languages from the languages-photo.json file. \n",
    "'''\n",
    "language_ref = dict()\n",
    "\n",
    "for photo in [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]:\n",
    "    photo_folder = os.path.join(base_path, photo)\n",
    "    \n",
    "    with open(os.path.join(photo_folder,'languages-'+photo+\".json\"),'r') as f:\n",
    "        lang = json.load(f)\n",
    "    \n",
    "    languages = []\n",
    "    \n",
    "    for iterkey,items1 in lang.items():\n",
    "        \n",
    "        for id_, lan_items in items1.items():\n",
    "            language = lan_items[0] # the language identifier also outputs the probability that the guess is right. We don't need that so we discard it here\n",
    "            language_ref.update({id_:language})\n",
    "                \n",
    "'''\n",
    "Import texts from parsed_text.json file in iteration folders. N.B.: this means that for every iteration one .json with texts is constructed.\n",
    "Hence the extra loop. This loop also combines all the information in one object. In this process we also extract the so-called Top Level\n",
    "Domain (TLD), for example www.facebook.com in the case of the URL www.facebook.com/user/something. \n",
    "'''\n",
    "text_ref = dict()\n",
    "\n",
    "for photo in [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]:\n",
    "    photo_folder = os.path.join(base_path, photo)\n",
    "    num_iterations = [fol for fol in os.listdir(photo_folder) if os.path.isdir(os.path.join(photo_folder,fol)) and \"source\" not in fol and \"context\" not in fol]\n",
    "    num_iterations = len(num_iterations)\n",
    "\n",
    "    start_iter = 1\n",
    "    range_iter = [str(i) for i in list(range(1,num_iterations + 1))]\n",
    "\n",
    "    folder_base = os.path.join(base_path,photo,photo)\n",
    "\n",
    "    for iteration in range_iter:\n",
    "        fn = os.path.join(folder_base + \"_\" +str(iteration),\"txt\", \"parsed_text.json\")\n",
    "        with open(fn) as fp:\n",
    "            pages = json.load(fp)\n",
    "            \n",
    "        for identifier,sentences in pages.items():\n",
    "            \n",
    "            sentences = [s.replace(\"\\n\",\"\").lower() for s in sentences]\n",
    "            sentences = [re.sub(' +', ' ', s) for s in sentences]\n",
    "\n",
    "            url = identifier.split('html_')[-1]\n",
    "            id_ = identifier.split('/html/')[1].split('.html_')[0]\n",
    "            if url in dates_ref.keys():\n",
    "                date = dates_ref[url]\n",
    "            else:\n",
    "                date = \"na\"\n",
    "            \n",
    "            text_ref.update({identifier:dict()})\n",
    "            text_ref[identifier].update({\"photo\":photo,\"url\":url,\"identifier\":id_,\"date\":date,\"language\":language_ref[url],\"topleveldomain\":URL.from_string(url).domain(),\"category\":category_ref[URL.from_string(url).domain()],\"sentences\":\"||\".join(sentences)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we transform the constructed dictionary is a DataFrame that can be exported to a .csv file\n",
    "'''\n",
    "\n",
    "df = pd.DataFrame.from_dict(text_ref,orient='index').reset_index()\n",
    "df.columns = [\"path\",\"photo\",\"url\",\"identifier\",\"date\",\"language\",\"topleveldomain\",\"category\",\"sentences\"]\n",
    "df.to_csv('path/to/datafolder/data-full.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Diachronic Frequency\n",
    "\n",
    "One of the first things to inspect is the distribution of the webpages over the years. When were most pages published? Can we identify peaks, or gaps? With the data stored in a .csv we can relatively easily generate visualizations that provide an insight into the data. Below we first aggregate the number of webpages per year and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('path/to/datafolder/data-full.csv')\n",
    "\n",
    "subset = data[['photo','date']] # get the photo and date columns\n",
    "subset['count'] = 1 # add a count column, because every row is one observation\n",
    "\n",
    "# loop over the dates and if the dates is not \"na\" extract the year\n",
    "\n",
    "subset['year'] = ''\n",
    "\n",
    "for c,i in enumerate(subset['date']):\n",
    "    \n",
    "    if \"na\" not in str(i):\n",
    "        year = str(i)[0:4]\n",
    "        subset['year'][c] = year\n",
    "    else:\n",
    "        subset['year'][c] = \"na\"\n",
    "        \n",
    "# Remove all observations with \"na\"\n",
    "subset = subset[~subset['year'] == \"na\"]\n",
    "\n",
    "# Group by photo and year\n",
    "\n",
    "subset = subset.groupby(['photo','year']).sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Top Level Domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Language Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = dict()\n",
    "for photo in [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]:\n",
    "    photo_folder = os.path.join(base_path, photo)\n",
    "    \n",
    "    with open(os.path.join(photo_folder,'languages-'+photo+\".json\"),'r') as f:\n",
    "        lang = json.load(f)\n",
    "    \n",
    "    languages = []\n",
    "    \n",
    "    for iterkey,items1 in lang.items():\n",
    "        languages += [v[0] for k,v in items1.items()]\n",
    "    \n",
    "    languages = dict(Counter(languages))\n",
    "    #pretty_title = refdf_title[photo]\n",
    "    d_.update({photo:languages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
