{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from boilerpipe.extract import Extractor\n",
    "import codecs\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C://Users//Ruben//Documents/GitHub/ReACT_GCV/code/work/scrape_environment/image_npg_1/html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C://Users//Ruben//Documents/GitHub/ReACT_GCV/code/work/scrape_environment/image_npg_1/html/results.txt') as f:\n",
    "    resdf = f.readlines()\n",
    "    resdf = [l.split('|') for l in resdf]\n",
    "    resdf = [(r[0].split('html')[1][1:] + \".html\",r[1].replace('\\n','')) for r in resdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_html = [i for i in os.listdir(os.getcwd()) if \".html\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParserBoilerArticle(html_object):\n",
    "    extractor = Extractor(extractor='ArticleSentencesExtractor', html=html_object)\n",
    "    sents = extractor.getText()\n",
    "    sents = list(nlp(sents).sents)\n",
    "    return sents\n",
    "\n",
    "def ParserBoilerDefault(html_object):\n",
    "    extractor = Extractor(extractor='DefaultExtractor', html=html_object)\n",
    "    sents = extractor.getText()\n",
    "    sents = list(nlp(sents).sents)\n",
    "    return sents\n",
    "\n",
    "def ParserBoilerEverything(html_object):\n",
    "    extractor = Extractor(extractor='DefaultExtractor', html=html_object)\n",
    "    sents = extractor.getText()\n",
    "    sents = list(nlp(sents).sents)\n",
    "    return sents\n",
    "\n",
    "def ParserRaw(html_object):\n",
    "    soup = BeautifulSoup(html_object, \"html.parser\")\n",
    "    [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "    text = soup.getText()\n",
    "    text = [t for t in text if t]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "line_len = pd.DataFrame()\n",
    "\n",
    "for c,i in enumerate(list_html):\n",
    "    with codecs.open(i,'r',encoding='utf-8',errors='ignore') as f:\n",
    "        html_object = f.read()\n",
    "    sents = ParserBoilerArticle(html_object)\n",
    "    if len(sents) < 2:\n",
    "        sents = ParserBoilerDefault(html_object)\n",
    "    if len(sents) < 2:\n",
    "        sents = ParserBoilerEverything(html_object)\n",
    "    if len(sents) < 2:\n",
    "        sents = ParserRaw(html_object)\n",
    "        \n",
    "        \n",
    "        \n",
    "    line_len = line_len.append(pd.DataFrame([i,len(sents),c,sents]).T)\n",
    "    \n",
    "line_len.columns = ['fn','length','ind','sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "for c,i in enumerate(line_len['fn']):\n",
    "    urls.append([x for x in resdf if x[0] == str(i)][0][1])\n",
    "line_len['url'] = urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in line_len[line_len['length'] == 0]['url']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[The girl in the photo: Kim Phuc has laser treatment for napalm burns – video\n",
      "Kim Phuc arrives in Miami for dermatology treatment to reduce the pain she suffers from burns incurred in 1972 when she was nine years old., Phuc was photographed after being burnt in a napalm attack during the Vietnam war., Nick Ut, who took the photo, remains in touch and accompanies Phuc to her laser treatment, which will continue for several months\n",
      "]\n",
      "[Dziewczynka ze synnego zdjcia z Wietnamu po 40 latach pozbywa si blu., Usunie blizny po poparzeniach napalmem\n",
      "Nick Ut / AP (AP Photo/Nick Ut, File)\n",
      "Zdjcie 9-letniej Kim Phuc, uciekajcej ze swoim bratem przed samolotami wojsk Wietnamu Poudniowego., W Trang Bang, rodzinnej wiosce dziewczynki, dokonano pomykowego nalotu z uyciem poncego napalmu.\n",
      "]\n",
      "\n",
      "\n",
      "[挪威最有名的報紙Aftenposten的總編輯Espen Egil Hansen更出頭版公開信，公然指責他藉著Facebook成為了一個全球性的分享新聞資訊的平台，濫用權力，嚴重打擊民主社會的言論自由，令整個原應以民主為中心的社交平台變為一個獨裁的媒介, 。\n",
      ", 標籤\n",
      "]\n",
      "[Telemetro es un canal de MEDCOM Panamá | Copyright © 2019., Todos los derechos reservados.\n",
      "]\n",
      "[Checking your browser before accessing bowenpress.com.\n",
      ", This process is automatic., Your browser will redirect to your requested content shortly.\n",
      "]\n",
      "[Excellente nouvelle, vous désirez découvrir Le Soir + et disposez déjà d'un identifiant et d'un mot de passe.\n",
      ", Il ne vous reste plus qu'à faire votre choix entre un abonnement avec un mois offert ou un accès 24h.\n",
      "]\n",
      "[Telemetro es un canal de MEDCOM Panamá | Copyright © 2019., Todos los derechos reservados.\n",
      "]\n",
      "[28/10/2019\n",
      ", 30/08/2019\n",
      "27/06/2019\n",
      "]\n",
      "[9., September 2016                                 12:46                                                                     -, Aktualisiert                                                                             12:48\n",
      "]\n",
      "[The page you were looking for could not be found., It might have been removed, renamed, or did not exist in the first place.\n",
      "]\n",
      "\n",
      "[An appropriate representation of the requested resource could not be found on this server., This error was generated by Mod_Security.\n",
      "]\n",
      "\n",
      "[15/01/2020\n",
      ", 14/01/2020\n",
      "8/01/2020\n",
      "]\n",
      "[Το επικό βλέμμα ενός σκύλου αφού... κατέστρεψε το σαλόνι!\n",
      ", Tο σαλόνι ήταν γεμάτο... βαμβάκια από τα μαξιλάρια του καναπέ!\n",
      ", Μία δυσάρεστη έκπληξη περίμενε την ιδιοκτήτρια ενός άτακτου τετράποδου μόλις άνοιξε την πόρτα του σπιτιού του.\n",
      "]\n",
      "[An appropriate representation of the requested resource could not be found on this server., This error was generated by Mod_Security.\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in line_len[line_len['length'] < 4]['sents']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0    [(An, appropriate, representation, of, the, re...\\nName: sents, dtype: object'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(line_len[line_len['fn'] == \"c90682b3-006b-4d50-8ad5-5e7f06f60e44.html\"]['sents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
