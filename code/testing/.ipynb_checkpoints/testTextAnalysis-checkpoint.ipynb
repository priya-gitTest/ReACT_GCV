{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "import os,sys\n",
    "from urllib.parse import urlparse\n",
    "from getImagesFunctions import *\n",
    "from getDataFunctions import *\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import random\n",
    "import langid\n",
    "import re\n",
    "import ast\n",
    "import concurrent\n",
    "import htmldate\n",
    "import time\n",
    "import uuid\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "import sys\n",
    "sys.modules['__main__'].__file__ = 'ipython'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1,2):\n",
    "    with open('C:/Users/Ruben/Documents/GitHub/ReACT_GCV/code/work/scrape_environment/image_npg_{}/html/results.txt'.format(n)) as f:\n",
    "        urls = f.readlines()\n",
    "        urls = [u.split('|') for u in urls]\n",
    "        urls = [[os.path.join('C:/Users/Ruben/Documents/GitHub/ReACT_GCV/code/work/scrape_environment/image_npg_{}/html'.format(n), u[0]) + \".html\", u[1].replace('\\n','')] for u in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArticleText(url_element, preprocess=True):\n",
    "    fn = url_element[0]\n",
    "    url = url_element[1]\n",
    "    \n",
    "    a = Article(url)\n",
    "\n",
    "    with open(fn, 'rb') as fh:\n",
    "        a.html = fh.read()\n",
    "    a.download_state = 2\n",
    "    a.parse()\n",
    "    text = a.text\n",
    "    \n",
    "    if preprocess == True:\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n','').split('.')\n",
    "        #text = [t for t in text if t]\n",
    "        text = [word_tokenize(s) for s in text]\n",
    "        text = [s for s in text if len(s) > 1]\n",
    "        \n",
    "    return {fn.split('/')[-1].split('\\\\')[1]:text}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    manager = multiprocessing.Manager()\n",
    "    dict_ = manager.dict()\n",
    "    jobs = []\n",
    "    for u in urls:\n",
    "        p = multiprocessing.Process(target=ArticleText, args=(u,dict_))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for proc in jobs:\n",
    "        proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    p = Pool(processes=20)\n",
    "    data = p.map(ArticleText, urls)\n",
    "    p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
